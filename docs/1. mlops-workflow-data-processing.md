# 🧹 Tutorial MLOps: Data Processing (Limpieza de Datos)

## 📋 Tabla de Responsabilidades

| Entregable                   | Responsable                   |
| ---------------------------- | ----------------------------- |
| 📓 Notebooks de exploración  | Científico/Ingeniero de datos |
| 🧹 Scripts de limpieza       | Ingeniero de MLOps            |
| 🔧 Scripts modulares         | Ingeniero de MLOps            |
| 🌐 API REST (FastAPI)        | Ingeniero de MLOps            |
| 📱 App prototipo (Streamlit) | Ingeniero de MLOps            |
| 🐳 Dockerfile + Compose      | Ingeniero de MLOps            |

# 📘 MLOps Workflow: Data Processing desde Notebook a Script

## 🌐 ¿Cómo ve cada rol los datos?

Imagina que formas parte de un equipo multifuncional. Todos trabajan con los mismos datos, pero desde perspectivas distintas:

- 👷‍♂️ **DevOps Engineer**: piensa en almacenamiento escalable, confiable e infraestructura. Se pregunta: ¿cómo aprovisiono esto con Terraform? ¿cómo lo monitoreo?
- 🧹 **Data Engineer (tú)**: se enfoca en limpiar, procesar, validar y dejar todo listo para otros roles.
- 🔬 **Data Scientist**: ve los datos como un tesoro. Quiere analizarlos y entrenar modelos predictivos.

---

## 🧑‍💻 ¿Qué hace un Ingeniero de Datos?

Tu misión aquí es **garantizar la calidad de los datos**. Recuerda: un modelo no puede ser mejor que los datos que recibe.

### 🛠️ Responsabilidades clave

- Descubrir y reunir datos de múltiples fuentes (transacciones, logs, APIs, Kaggle).
- Procesarlos para que sean consistentes, completos y útiles:
  - Detectar y eliminar duplicados.
  - Corregir tipos de datos.
  - Eliminar valores inválidos o extremos.
  - Validar reglas lógicas (ej. que el precio sea mayor a 0).

---

## 📓 Paso 1: Exploración en Notebook

Usamos herramientas como `pandas`, `matplotlib`, `seaborn`, `numpy` para hacer análisis exploratorio:

```python
import pandas as pd
import matplotlib.pyplot as plt

# Carga inicial
df = pd.read_csv('data/raw/house_data.csv')
print(df.shape)
df.head()

# Revisión de tipos y nulos
df.info()
df.isna().sum()

# Estadísticas básicas
df.describe()

# Distribución de precios
df['price'].hist()

# Detectar outliers
plt.boxplot(df['sqft'])
plt.show()
```

> 🔍 Esta fase te permite conocer el dataset y definir las reglas que aplicarás luego en el script.

---

## 🧼 Paso 2: Automatización con Script (`run_processing.py`)

Una vez definido lo que quieres limpiar o corregir, lo transformas en un script ejecutable como este:

```bash
python src/data/run_processing.py   --input data/raw/house_data.csv   --output data/processed/cleaned_house_data.csv
```

### 🧠 Ejemplo de contenido del script:

```python
# run_processing.py
import pandas as pd
import numpy as np

# 1. Cargar datos
raw = pd.read_csv(input_file)

# 2. Eliminar duplicados
data = raw.drop_duplicates()

# 3. Eliminar valores extremos
data = data[(data['sqft'] > 300) & (data['price'] > 10000)]

# 4. Eliminar valores negativos
data = data[data.select_dtypes(include=[np.number]) >= 0].dropna()

# 5. Limpiar nombres de columnas
data.columns = data.columns.str.lower().str.replace(' ', '_')

# 6. Guardar datos limpios
data.to_csv(output_file, index=False)
```

> ⚙️ Este script automatiza todo lo que descubriste en el análisis exploratorio.

---

## ✅ Resultado del Proceso

- **Entrada:** `data/raw/house_data.csv` con 84 filas.
- **Limpieza aplicada:**
  - Se eliminaron 7 outliers (valores extremos).
  - No había valores nulos, pero de existir se imputan o eliminan.
  - Se homogeneizaron los nombres de las columnas.
- **Salida:** `data/processed/cleaned_house_data.csv` con 77 filas.

---

## 👩‍🔬 Rol del Científico de Datos en esta etapa

En esta fase nos ponemos en el lugar de un científico de datos. Su objetivo no es limpiar datos ni desplegar modelos, sino **entender los datos a fondo antes de modelar**.

El científico de datos utiliza herramientas como:

- Jupyter Notebooks o JupyterLab
- Entornos virtuales de Python
- Bibliotecas como `pandas`, `numpy`, `matplotlib`, `seaborn`
- Plataformas como **Databricks** cuando necesita recursos computacionales a escala

No suele preocuparse por contenedores, CI/CD o Git. Solo quiere:

> “Un cuaderno, los datos y un kernel que funcione”

---

## 🧪 ¿Qué es el EDA?

El **Análisis Exploratorio de Datos (EDA)** es una etapa previa al modelado que permite:

- Entender la **estructura estadística de los datos**
- Visualizar la **distribución de las variables**
- Identificar correlaciones entre atributos
- Detectar sesgos, desbalance o problemas ocultos

El científico de datos recibe los **datos limpios** del ingeniero de datos y explora:

- Valores máximos y mínimos
- Medias, medianas, desviaciones
- Distribución de la variable objetivo
- Relaciones entre columnas

---

## 📘 Ejemplo aplicado: dataset de precios de casas

### 1. Cargar datos

```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('../data/raw/house_data.csv')
df.shape  # filas y columnas
df.head() # primeras observaciones
```

### 2. Estadísticas y nulos

```python
df.info()
df.describe()
df.isna().sum()
```

### 3. Visualizar distribución del precio

```python
df['price'].hist(bins=30)
```

### 4. Correlación entre variables

La matriz de correlación (heatmap) nos permite visualizar rápidamente qué variables numéricas están más relacionadas entre sí. Una correlación cercana a 1 indica una relación lineal directa fuerte, mientras que una correlación cercana a -1 indica una relación inversa. Esto es útil para detectar redundancias (colinealidad) o identificar variables altamente predictivas (como `sqft` y `price`).

```python
plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap="coolwarm")
```

### 5. Relación precio vs pies cuadrados

Este análisis visual muestra cómo se relaciona la superficie de la casa (en pies cuadrados) con su precio. Es común encontrar una **relación lineal positiva**, lo que significa que a mayor superficie, mayor suele ser el precio. El diagrama de dispersión permite ver esa tendencia general, así como posibles valores atípicos o desviaciones que podrían necesitar revisión.

```python
sns.scatterplot(data=df, x="sqft", y="price")
```

### 6. Casas por ubicación (categoría)

```python
df['location'].value_counts().plot(kind="bar")
```

### 7. Precio promedio por número de dormitorios

Este gráfico muestra cómo varía el precio promedio de las viviendas en función del número de dormitorios. Es útil para identificar patrones como: ¿las casas con más dormitorios cuestan sistemáticamente más? ¿Hay un punto donde el precio se estabiliza o incluso baja? Esta observación puede ayudarte a validar hipótesis o ajustar variables categóricas y ordinales.

```python
df.groupby("bedrooms")['price'].mean().plot(kind="bar")
```

---

## 🎯 ¿Para qué sirve este análisis?

- Detectar **colinealidades** entre variables (ej. sqft y precio)
- Entender **cuáles atributos son más influyentes**
- Saber si se necesitan nuevas features o si las existentes ya explican bien la salida
- Ver si hay **desbalance** entre categorías (ubicaciones poco representadas)

---

## 🧠 Conclusión y siguientes pasos

El EDA te ayuda a formular preguntas como:

- ¿Puedo predecir el precio solo con `sqft`, `bedrooms` y `location`?
- ¿Necesito combinar variables (ej. ratio habitaciones/baños)?
- ¿Faltan datos para alguna clase o condición?

---

## ⚙️ Paso 3: Automatización del Script en GitHub Actions

Para llevar todo a un entorno real de integración continua, puedes configurar tu flujo de procesamiento de datos como un **workflow automatizado** con GitHub Actions:

### 🧾 `.github/workflows/mlops-pipeline.yml`

```yaml
name: MLOps Pipeline

on:
  workflow_dispatch:
    inputs:
      run_all:
        description: "Run all jobs"
        required: false
        default: "true"

jobs:
  data-processing:
    name: Data Processing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Process data
        run: |
          python src/data/run_processing.py --input data/raw/house_data.csv --output data/processed/cleaned_house_data.csv
```

> 🚀 Esto permite que el procesamiento de datos se ejecute automáticamente cada vez que lo dispares manualmente o se cree una nueva versión.

---

## 🧭 Siguiente paso

Tu siguiente destino es crear un **pipeline robusto de feature engineering**. Irás de un notebook exploratorio a un script modular (`engineer.py`) que puede ejecutarse en producción.

📄 **Continuar en**: `mlops-workflow-feature-engineering.md`
