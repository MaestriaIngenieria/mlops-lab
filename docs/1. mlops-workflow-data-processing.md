# 🧹 Tutorial MLOps: Data Processing (Limpieza de Datos)

## 📋 Tabla de Responsabilidades

| Entregable                   | Responsable                   |
| ---------------------------- | ----------------------------- |
| 📓 Notebooks de exploración  | Científico/Ingeniero de datos |
| 🧹 Scripts de limpieza       | Ingeniero de MLOps            |
| 🔧 Scripts modulares         | Ingeniero de MLOps            |
| 🌐 API REST (FastAPI)        | Ingeniero de MLOps            |
| 📱 App prototipo (Streamlit) | Ingeniero de MLOps            |
| 🐳 Dockerfile + Compose      | Ingeniero de MLOps            |

# 📘 MLOps Workflow: Data Processing desde Notebook a Script

## 🌐 ¿Cómo ve cada rol los datos?

Imagina que formas parte de un equipo multifuncional. Todos trabajan con los mismos datos, pero desde perspectivas distintas:

- 👷‍♂️ **DevOps Engineer**: piensa en almacenamiento escalable, confiable e infraestructura. Se pregunta: ¿cómo aprovisiono esto con Terraform? ¿cómo lo monitoreo?
- 🧹 **Data Engineer (tú)**: se enfoca en limpiar, procesar, validar y dejar todo listo para otros roles.
- 🔬 **Data Scientist**: ve los datos como un tesoro. Quiere analizarlos y entrenar modelos predictivos.

---

## 🧑‍💻 ¿Qué hace un Ingeniero de Datos?

Tu misión aquí es **garantizar la calidad de los datos**. Recuerda: un modelo no puede ser mejor que los datos que recibe.

### 🛠️ Responsabilidades clave

- Descubrir y reunir datos de múltiples fuentes (transacciones, logs, APIs, Kaggle).
- Procesarlos para que sean consistentes, completos y útiles:
  - Detectar y eliminar duplicados.
  - Corregir tipos de datos.
  - Eliminar valores inválidos o extremos.
  - Validar reglas lógicas (ej. que el precio sea mayor a 0).

---

## 📓 Paso 1: Exploración en Notebook

Usamos herramientas como `pandas`, `matplotlib`, `seaborn`, `numpy` para hacer análisis exploratorio:

```python
import pandas as pd
import matplotlib.pyplot as plt

# Carga inicial
df = pd.read_csv('data/raw/house_data.csv')
print(df.shape)
df.head()

# Revisión de tipos y nulos
df.info()
df.isna().sum()

# Estadísticas básicas
df.describe()

# Distribución de precios
df['price'].hist()

# Detectar outliers
plt.boxplot(df['sqft'])
plt.show()
```

> 🔍 Esta fase te permite conocer el dataset y definir las reglas que aplicarás luego en el script.

---

## 🧼 Paso 2: Automatización con Script (`run_processing.py`)

Una vez definido lo que quieres limpiar o corregir, lo transformas en un script ejecutable como este:

```bash
python run_processing.py \
  --input data/raw/house_data.csv \
  --output data/processed/cleaned_house_data.csv
```

### 🧠 Ejemplo de contenido del script:

```python
# run_processing.py
import pandas as pd
import numpy as np

# 1. Cargar datos
raw = pd.read_csv(input_file)

# 2. Eliminar duplicados
data = raw.drop_duplicates()

# 3. Eliminar valores extremos
data = data[(data['sqft'] > 300) & (data['price'] > 10000)]

# 4. Eliminar valores negativos
data = data[data.select_dtypes(include=[np.number]) >= 0].dropna()

# 5. Limpiar nombres de columnas
data.columns = data.columns.str.lower().str.replace(' ', '_')

# 6. Guardar datos limpios
data.to_csv(output_file, index=False)
```

> ⚙️ Este script automatiza todo lo que descubriste en el análisis exploratorio.

---

## ✅ Resultado del Proceso

- **Entrada:** `data/raw/house_data.csv` con 84 filas.
- **Limpieza aplicada:**
  - Se eliminaron 7 outliers (valores extremos).
  - No había valores nulos, pero de existir se imputan o eliminan.
  - Se homogeneizaron los nombres de las columnas.
- **Salida:** `data/processed/cleaned_house_data.csv` con 77 filas.

---

---

## ⚙️ Paso 3: Automatización del Script en GitHub Actions

Para llevar todo a un entorno real de integración continua, puedes configurar tu flujo de procesamiento de datos como un **workflow automatizado** con GitHub Actions:

### 🧾 `.github/workflows/mlops-pipeline.yml`

```yaml
name: MLOps Pipeline

on:
  workflow_dispatch:
    inputs:
      run_all:
        description: "Run all jobs"
        required: false
        default: "true"
      run_data_processing:
        description: "Run data processing job"
        required: false
        default: "false"

  release:
    types: [created]
    branches: [main]
    tags: ["v*.*.*"]

jobs:
  data-processing:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Process data
        run: |
          python src/data/run_processing.py --input data/raw/house_data.csv --output data/processed/cleaned_house_data.csv
```

> 🚀 Esto permite que el procesamiento de datos se ejecute automáticamente cada vez que lo dispares manualmente o se cree una nueva versión.

---

## 🧭 Siguiente paso

Tu siguiente destino es crear un **pipeline robusto de feature engineering**. Irás de un notebook exploratorio a un script modular (`engineer.py`) que puede ejecutarse en producción.

📄 **Continuar en**: `mlops-workflow-feature-engineering.md`
