# 🧠 Tutorial MLOps: Feature Engineering de Producción

## 📋 Tabla de Responsabilidades

| Entregable                        | Responsable         | Status               |
| --------------------------------- | ------------------- | -------------------- |
| 📓 Exploración de características | Científico de datos | ✅ Completo          |
| 🔧 Pipeline de transformación     | Ingeniero de MLOps  | 🎯 **Este tutorial** |
| 🤖 Preprocessor serializado       | Ingeniero de MLOps  | 🎯 **Este tutorial** |
| 📊 Features listas para ML        | Ingeniero de MLOps  | 🎯 **Este tutorial** |
| 🔄 Script reutilizable            | Ingeniero de MLOps  | 🎯 **Este tutorial** |

---

# 📘 MLOps Workflow: Feature Engineering desde Notebook a Script

### 📚 **¿Qué recibe el MLOps Engineer?**

El científico de datos entrega un notebook `02_feature_engineering.ipynb` con:

```python
# 📓 Código exploratorio del Data Scientist
import pandas as pd
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns

# Carga y exploración
df = pd.read_csv("../data/processed/cleaned_house_data.csv")
print(f"Dataset shape: {df.shape}")
df.head()

# Feature Engineering exploratorio
df['house_age'] = 2025 - df['year_built']
df['price_per_sqft'] = df['price'] / df['sqft']
df['bed_bath_ratio'] = df['bedrooms'] / df['bathrooms']

# Análisis de correlaciones
plt.figure(figsize=(12, 8))
correlation_matrix = df[['price', 'sqft', 'house_age', 'price_per_sqft', 'bed_bath_ratio']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# Encoding categórico manual
df_encoded = pd.get_dummies(df, columns=['location', 'condition'])

# Normalización básica
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
numeric_cols = ['sqft', 'bedrooms', 'bathrooms', 'house_age', 'price_per_sqft', 'bed_bath_ratio']
df_encoded[numeric_cols] = scaler.fit_transform(df_encoded[numeric_cols])
```

### ⚡ **Transformación a Pipeline de Producción**

El MLOps Engineer convierte esto en `src/features/engineer.py`:

```python
# src/features/engineer.py
import pandas as pd
import numpy as np
from datetime import datetime
import logging
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import joblib

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('feature-engineering')

def create_features(df):
    """Create new features from existing data."""
    logger.info("Creating new features")

    # Make a copy to avoid modifying the original dataframe
    df_featured = df.copy()

    # Calculate house age
    current_year = datetime.now().year
    df_featured['house_age'] = current_year - df_featured['year_built']
    logger.info("Created 'house_age' feature")

    # Price per square foot
    df_featured['price_per_sqft'] = df_featured['price'] / df_featured['sqft']
    logger.info("Created 'price_per_sqft' feature")

    # Bedroom to bathroom ratio
    df_featured['bed_bath_ratio'] = df_featured['bedrooms'] / df_featured['bathrooms']
    # Handle division by zero
    df_featured['bed_bath_ratio'] = df_featured['bed_bath_ratio'].replace([np.inf, -np.inf], np.nan)
    df_featured['bed_bath_ratio'] = df_featured['bed_bath_ratio'].fillna(0)
    logger.info("Created 'bed_bath_ratio' feature")

    # Do NOT one-hot encode categorical variables here; let the preprocessor handle it
    return df_featured

def create_preprocessor():
    """Create a preprocessing pipeline."""
    logger.info("Creating preprocessor pipeline")

    # Define feature groups
    categorical_features = ['location', 'condition']
    numerical_features = ['sqft', 'bedrooms', 'bathrooms', 'house_age', 'price_per_sqft', 'bed_bath_ratio']

    # Preprocessing for numerical features
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean'))
    ])

    # Preprocessing for categorical features
    categorical_transformer = Pipeline(steps=[
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Combine preprocessors in a column transformer
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )

    return preprocessor

def run_feature_engineering(input_file, output_file, preprocessor_file):
    """Full feature engineering pipeline."""
    # Load cleaned data
    logger.info(f"Loading data from {input_file}")
    df = pd.read_csv(input_file)

    # Create features
    df_featured = create_features(df)
    logger.info(f"Created featured dataset with shape: {df_featured.shape}")

    # Create and fit the preprocessor
    preprocessor = create_preprocessor()
    X = df_featured.drop(columns=['price'], errors='ignore')  # Features only
    y = df_featured['price'] if 'price' in df_featured.columns else None  # Target column (if available)
    X_transformed = preprocessor.fit_transform(X)
    logger.info("Fitted the preprocessor and transformed the features")

    # Save the preprocessor
    joblib.dump(preprocessor, preprocessor_file)
    logger.info(f"Saved preprocessor to {preprocessor_file}")

    # Save fully preprocessed data
    df_transformed = pd.DataFrame(X_transformed)
    if y is not None:
        df_transformed['price'] = y.values
    df_transformed.to_csv(output_file, index=False)
    logger.info(f"Saved fully preprocessed data to {output_file}")

    return df_transformed

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Feature engineering for housing data.')
    parser.add_argument('--input', required=True, help='Path to cleaned CSV file')
    parser.add_argument('--output', required=True, help='Path for output CSV file (engineered features)')
    parser.add_argument('--preprocessor', required=True, help='Path for saving the preprocessor')

    args = parser.parse_args()

    run_feature_engineering(args.input, args.output, args.preprocessor)
```

## 🔑 **Diferencias Clave: Exploración vs Script**

### 📓 **Código del Data Scientist (Notebook)**

```python
# ❌ Código exploratorio - No apto para producción
df['house_age'] = 2025 - df['year_built']  # Año hardcodeado
df['bed_bath_ratio'] = df['bedrooms'] / df['bathrooms']  # Sin manejo de división por 0
df_encoded = pd.get_dummies(df, columns=['location'])  # Sin handle_unknown
scaler.fit_transform(df[numeric_cols])  # Fit y transform juntos (data leakage)
```

### 🔧 **Código del MLOps Engineer (Script)**

```python
# ✅ Código de producción - Robusto y escalable
class HouseFeaturesEngineer(BaseEstimator, TransformerMixin):
    def __init__(self, current_year=None):
        self.current_year = current_year or datetime.now().year  # ✅ Configurable

    def transform(self, X):
        X_transformed['bed_bath_ratio'] = X['bedrooms'] / X['bathrooms'].replace(0, np.nan)  # ✅ Manejo de división por 0

Pipeline([
    ('feature_engineer', HouseFeaturesEngineer()),  # ✅ Separación clara
    ('preprocessor', ColumnTransformer([
        ('onehot', OneHotEncoder(handle_unknown='ignore'))  # ✅ Manejo de categorías nuevas
    ]))
])
```

## 🚀 **Ejecutar el Script de Feature Engineering**

### 1. **Crear estructura de directorios:**

```bash
mkdir -p data/processed models/trained logs
```

### 2. **Ejecutar pipeline completo:**

```bash
python src/features/engineer.py   --input data/processed/cleaned_house_data.csv   --output data/processed/featured_house_data.csv   --preprocessor models/trained/preprocessor.pkl
```

### 3. **Output esperado:**

```
2025-07-26 09:35:28,293 - feature-engineering - INFO - Loading data from data/processed/cleaned_house_data.csv
2025-07-26 09:35:28,295 - feature-engineering - INFO - Creating new features
2025-07-26 09:35:28,295 - feature-engineering - INFO - Created 'house_age' feature
2025-07-26 09:35:28,296 - feature-engineering - INFO - Created 'price_per_sqft' feature
2025-07-26 09:35:28,296 - feature-engineering - INFO - Created 'bed_bath_ratio' feature
2025-07-26 09:35:28,296 - feature-engineering - INFO - Created featured dataset with shape: (77, 10)
2025-07-26 09:35:28,296 - feature-engineering - INFO - Creating preprocessor pipeline
2025-07-26 09:35:28,303 - feature-engineering - INFO - Fitted the preprocessor and transformed the features
2025-07-26 09:35:28,304 - feature-engineering - INFO - Saved preprocessor to models/trained/preprocessor.pkl
2025-07-26 09:35:28,306 - feature-engineering - INFO - Saved fully preprocessed data to data/processed/featured_house_data.cs
```

## 📦 **Archivos Generados**

### 1. **`featured_house_data.csv`**

```csv
# Datos transformados con todas las características engineered
num__sqft,num__bedrooms,num__bathrooms,num__price_per_sqft,num__house_age,num__bed_bath_ratio,num__sqft_per_bedroom,num__is_new_house,num__is_large_house,cat__location_Downtown,cat__location_Mountain,cat__location_Rural,cat__location_Suburb,cat__location_Urban,cat__location_Waterfront,cat__condition_Excellent,cat__condition_Fair,cat__condition_Good,cat__condition_Poor,cat__location_category_Premium,cat__location_category_Rural,cat__location_category_Suburban,cat__location_category_Urban,price
-0.23,0.14,-0.45,1.2,0.67,0.89,-0.12,0,1,0,0,0,1,0,0,0,0,1,0,0,0,1,0,250000
```

### 2. **`preprocessor.pkl`**

- Pipeline completo de sklearn serializado
- Incluye HouseFeaturesEngineer + ColumnTransformer
- Contiene todas las transformaciones aprendidas (scaling, encoding, etc.)
- Listo para usar en producción sin reentrenamiento

## 🎯 **¿Por qué este Script es Superior?**

### **🔄 Reutilización Perfecta:**

```python
# Entrenamiento
pipeline = joblib.load('models/trained/preprocessor.pkl')
X_train_transformed = pipeline.fit_transform(X_train)
model.fit(X_train_transformed, y_train)

# Producción (mismas transformaciones automáticamente)
X_new_transformed = pipeline.transform(X_new)
predictions = model.predict(X_new_transformed)
```

### **🛡️ Robustez de Producción:**

1. **Manejo de valores faltantes**: SimpleImputer con estrategias configurables
2. **Categorías nuevas**: OneHotEncoder con `handle_unknown='ignore'`
3. **División por cero**: Reemplazo seguro en ratios
4. **Escalabilidad**: Funciona con 1 fila o 1 millón de filas
5. **Versionado**: Pipelines serializados para control de versiones

### **📊 Características Creadas:**

| Característica     | Descripción               | Valor de Negocio      |
| ------------------ | ------------------------- | --------------------- |
| `house_age`        | Años desde construcción   | Depreciation modeling |
| `price_per_sqft`   | Precio por pie cuadrado   | Comparación de valor  |
| `sqft_per_bedroom` | Superficie por habitación | Space optimization    |
| `bed_bath_ratio`   | Ratio habitaciones/baños  | Layout efficiency     |

## ✅ **Beneficios de esta Transformación**

### **🎯 Para el MLOps Engineer:**

1. **🔄 Reproducibilidad**: Mismo resultado siempre
2. **⚡ Automatización**: Integrable en pipelines CI/CD
3. **🛡️ Robustez**: Manejo de errores y edge cases
4. **📈 Escalabilidad**: Procesa cualquier volumen
5. **🔧 Mantenibilidad**: Código modular y testeable
6. **🎯 Consistencia**: Entrenamiento = Producción

### **🎯 Para el Modelo ML:**

1. **📊 Más información**: 25 features vs 7 originales
2. **🎯 Mejor predicción**: Features específicas del dominio
3. **🔄 Transformaciones aprendidas**: No data leakage
4. **📈 Generalización**: Manejo robusto de datos nuevos

## 🧪 **Testing del Pipeline**

Automatización del Script en GitHub Actions

Para llevar todo a un entorno real de integración continua, puedes configurar tu flujo de procesamiento de datos como un **workflow automatizado** con GitHub Actions:

### 🧾 `.github/workflows/mlops-pipeline.yml`

```yaml
name: MLOps Pipeline

on:
  workflow_dispatch:
    inputs:
      run_all:
        description: "Run all jobs"
        required: false
        default: "true"

jobs:
  data-processing:
    name: Data Processing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Process data
        run: |
          python src/data/run_processing.py --input data/raw/house_data.csv --output data/processed/cleaned_house_data.csv

      - name: Upload processed data
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed/cleaned_house_data.csv

  feature-engineering:
    name: Feature Engineering
    needs: data-processing
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.11.9"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download processed data
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed/

      - name: Engineer features
        run: |
          python src/features/engineer.py --input data/processed/cleaned_house_data.csv --output data/processed/featured_house_data.csv --preprocessor models/trained/preprocessor.pkl
```

> 🚀 Esto permite que el procesamiento de datos se ejecute automáticamente cada vez que lo dispares manualmente o se cree una nueva versión.

---
